# ... existing code ...
#
# 新增文件: LAR-IQA/models/iqa/lar_iqa_model.py
# -------------------------------------------------
# LAR-IQA Dual-Branch KAN Network
# Author: Generated by ChatGPT
# Date: 2025-04-10
#
# 该模型复现了论文中的 LAR-IQA 框架: 一张输入图片经过『Downscaling』和『Cropping』两条支路,
# 分别提取特征并经由 KAN(或其替代实现)降采样后融合, 最终通过回归头预测主观质量分(MOS)。
#
# 设计要点:
# 1. 预训练图像编码器(默认 ResNet-50) —— 输出全局向量特征
# 2. KAN(如果环境中没有 efficient_kan, 使用轻量化全连接替代)
# 3. 可配置缩放因子 & 裁剪比例
# 4. 返回单一标量 MOS

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models, transforms

# -----------------------------------------------------------------------------
# KAN / fallback implementation
# -----------------------------------------------------------------------------
try:
    from efficient_kan import KAN  # 假设 efficient_kan 提供 1D 版本
    HAS_KAN = True
except ImportError:  # 如果环境没有 KAN, 使用简单 MLP 近似
    HAS_KAN = False

    class KAN(nn.Module):
        """简单的 MLP 替代, 保持接口一致"""

        def __init__(self, in_dim: int, hidden_dim: int = 128, depth: int = 2, dropout: float = 0.1):
            super().__init__()
            layers = [nn.Linear(in_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout)]
            for _ in range(depth - 1):
                layers += [nn.Linear(hidden_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout)]
            self.net = nn.Sequential(*layers)
            self.out_proj = nn.Linear(hidden_dim, in_dim)

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            return self.out_proj(self.net(x))

# -----------------------------------------------------------------------------
# 预训练编码器: 返回 1D 向量
# -----------------------------------------------------------------------------
class ImageEncoder(nn.Module):
    """封装 torchvision 预训练模型, 输出全局特征向量"""

    def __init__(self, backbone: str = "resnet50", pretrained: bool = True):
        super().__init__()
        self.backbone_name = backbone.lower()

        if self.backbone_name == "resnet50":
            weights = models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None
            model = models.resnet50(weights=weights)
            self.out_dim = model.fc.in_features  # 2048
            # 去掉 fc & avgpool, 仅保留卷积层
            self.features = nn.Sequential(*list(model.children())[:-2])  # [B, 2048, H/32, W/32]
        elif self.backbone_name.startswith("mobilenet"):
            weights = models.MobileNet_V3_Small_Weights.DEFAULT if pretrained else None
            model = models.mobilenet_v3_small(weights=weights)
            self.out_dim = 576
            self.features = model.features  # [B, 576, H/32, W/32]
        else:
            raise ValueError(f"Unsupported backbone: {backbone}")

        # 全局池化 -> 向量
        self.gap = nn.AdaptiveAvgPool2d(1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # 输出 shape [B, C]
        feat = self.features(x)
        feat = self.gap(feat).flatten(1)
        return feat  # [B, C]

# -----------------------------------------------------------------------------
# 主网络
# -----------------------------------------------------------------------------
class LARIQANetwork(nn.Module):
    """LAR-IQA 双分支网络 (Downscale + Crop)"""

    def __init__(
        self,
        backbone: str = "resnet50",
        pretrained: bool = True,
        downscale_factor: float = 0.5,
        crop_ratio: float = 0.5,
        hidden_dim: int = 512,
        kan_grid_size: int = 5,
        kan_spline_type: str = "bspline",
    ):
        super().__init__()
        self.downscale_factor = downscale_factor
        self.crop_ratio = crop_ratio
        self.kan_grid_size = kan_grid_size
        self.kan_spline_type = kan_spline_type

        # 共享同一编码器权重 (论文中两支路权重共享)
        self.encoder = ImageEncoder(backbone, pretrained)

        # Each latent feature passes through its own KAN down-sampler
        self.kan_a = KAN(self.encoder.out_dim, hidden_dim, depth=2)
        self.kan_s = KAN(self.encoder.out_dim, hidden_dim, depth=2)

        # Fusion + final regressor (another KAN)
        self.fusion_kan = KAN(self.encoder.out_dim, hidden_dim, depth=3)
        self.mos_head = nn.Linear(self.encoder.out_dim, 1)

    # ------------------------------------------------------------------
    # Helper functions for preprocessing
    # ------------------------------------------------------------------
    def _downscale(self, img: torch.Tensor) -> torch.Tensor:
        """Downscale image by a fixed factor using bilinear interpolation."""
        return F.interpolate(img, scale_factor=self.downscale_factor, mode="bilinear", align_corners=False)

    def _central_crop(self, img: torch.Tensor) -> torch.Tensor:
        """Crop the central region of the image."""
        _, _, h, w = img.size()
        ch, cw = int(h * self.crop_ratio), int(w * self.crop_ratio)
        top = (h - ch) // 2
        left = (w - cw) // 2
        return img[:, :, top : top + ch, left : left + cw]

    # ------------------------------------------------------------------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass.
        Args:
            x: Input tensor [B, 3, H, W]
        Returns:
            mos: Predicted MOS score, shape [B]
        """
        # Branch 1: Downscaling (Authentic)
        x_ds = self._downscale(x)
        feat_a = self.encoder(x_ds)
        feat_a = self.kan_a(feat_a)

        # Branch 2: Cropping (Synthetic)
        x_cp = self._central_crop(x)
        # 为了兼容编码器输入大小，将裁剪结果 resize 到与 downscale 相同大小
        x_cp = F.interpolate(x_cp, size=x_ds.shape[-2:], mode="bilinear", align_corners=False)
        feat_s = self.encoder(x_cp)
        feat_s = self.kan_s(feat_s)

        # Fuse (sum) & regression
        fused = feat_a + feat_s  # [B, C]
        fused = self.fusion_kan(fused)
        mos = self.mos_head(fused).squeeze(-1)  # [B]
        return mos

# -----------------------------------------------------------------------------
# Quick test
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    model = LARIQANetwork()
    dummy = torch.randn(2, 3, 256, 256)
    out = model(dummy)
    print("Output shape:", out.shape)  # Expected: [2]

# 定义 KANBlock 作为简单的卷积块（用于替代 KAN 处理 2D 特征图）
class KANBlock(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        # 使用简单的卷积层作为 KAN 的替代
        self.net = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
        )

    def forward(self, x):
        # 可以实现简单的残差连接
        return x + self.net(x)

# 定义加权特征融合模块
class WeightedFeatureFusion(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        # 通道注意力机制，来学习融合权重
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels * 2, in_channels * 2 // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels * 2 // reduction, 2, bias=False),
            nn.Softmax(dim=1)
        )
        
    def forward(self, feat_a, feat_b):
        batch_size, channels, _, _ = feat_a.shape
        
        # 拼接特征并通过全局平均池化
        feat_concat = torch.cat([
            self.avg_pool(feat_a).view(batch_size, channels),
            self.avg_pool(feat_b).view(batch_size, channels)
        ], dim=1)
        
        # 生成两个权重
        weights = self.fc(feat_concat)
        
        # 权重应用到整个特征图
        weight_a = weights[:, 0].view(batch_size, 1, 1, 1)
        weight_b = weights[:, 1].view(batch_size, 1, 1, 1)
        
        # 加权融合
        return weight_a * feat_a + weight_b * feat_b

# 定义简单的卷积自注意力模块
class SelfAttentionBlock(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1)) # Learnable scaling factor
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        batch_size, C, height, width = x.size()
        
        # Query, Key, Value
        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(batch_size, -1, width * height)
        proj_value = self.value_conv(x).view(batch_size, -1, width * height)
        
        # Attention map
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)
        
        # Apply attention
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, height, width)
        
        # Add back to original input with learnable scale
        out = self.gamma * out + x
        return out

class LARIQACrowdCounter(nn.Module):
    """LAR-IQA Crowd Counter with optional Multi-Scale Feature Fusion (MSFF)."""

    def __init__(
        self,
        backbone: str = "resnet50",
        pretrained: bool = True,
        downscale_factor: float = 0.5,
        crop_ratio: float = 0.5,
        hidden_dim: int = 256,
        kan_grid_size: int = 5,
        kan_spline_type: str = "bspline",
        use_msff: bool = False,  # 新增：是否启用多尺度特征融合
        # 新增参数
        fusion_method: str = "concat", # 特征融合方式
        use_attention: bool = False,   # 是否使用注意力
        attention_type: str = "self",  # 注意力类型
    ):
        super().__init__()
        self.kan_grid_size = kan_grid_size
        self.kan_spline_type = kan_spline_type
        self.downscale_factor = downscale_factor
        self.crop_ratio = crop_ratio
        self.use_msff = use_msff
        # 存储新参数
        self.fusion_method = fusion_method
        self.use_attention = use_attention
        self.attention_type = attention_type

        # 共享编码器，返回空间特征而非 GAP 向量
        self.encoder = ImageEncoder(backbone, pretrained)
        feat_dim = self.encoder.out_dim  # 通道数 (C)
        self.hidden_dim = hidden_dim

        # 降维方便后续处理
        self.reduce = nn.Conv2d(feat_dim, hidden_dim, kernel_size=1)

        # 多尺度特征融合相关
        if self.use_msff:
            # 针对 MobileNetV3 Large，提取3个stage特征
            # stage2: features[5] (C=40), stage3: features[11] (C=96), stage4: features[16] (C=576)
            self.msff_conv2 = nn.Conv2d(40, hidden_dim, 1)   # stage2
            self.msff_conv3 = nn.Conv2d(96, hidden_dim, 1)   # stage3
            self.msff_conv4 = nn.Conv2d(576, hidden_dim, 1)  # stage4 (修正: 576)
            # 融合后再降维
            self.msff_fuse = nn.Conv2d(hidden_dim * 3, hidden_dim, 1)
            # 为 MSFF 分支添加 KAN 块
            self.msff_kan = KANBlock(hidden_dim)
            # MSFF 分支也需要回归头和可能的上采样层
            self.msff_regressor = nn.Conv2d(hidden_dim, 1, kernel_size=1)

        # =======================================================
        # 添加基于新参数的模块 (占位符，后续需在 forward 中实现逻辑)
        # =======================================================
        # 注意力模块 (需要根据 attention_type 实现具体类型)
        if self.use_attention:
            if self.attention_type == 'self':
                self.attention = SelfAttentionBlock(hidden_dim)
            elif self.attention_type == 'cbam':
                # TODO: Implement or import CBAM module
                print(f"警告: CBAM attention type 未实现, 使用 Identity 代替.")
                self.attention = nn.Identity() # Placeholder for CBAM
            elif self.attention_type == 'cross':
                 # Cross-attention 通常需要两个输入，放在这里可能不合适
                 # 可能需要在 fusion 阶段实现
                print(f"警告: Cross attention type 在此位置未实现, 使用 Identity 代替.")
                self.attention = nn.Identity() # Placeholder for Cross Attention
            else: # none or other types
                print(f"注意: 未知的 attention_type '{self.attention_type}' 或未启用, 使用 Identity.")
                self.attention = nn.Identity()
        else:
            self.attention = nn.Identity()

        # KAN 块 (用卷积替代，如果需要 KAN，确保引入相关库)
        self.kan_a = KANBlock(hidden_dim)
        self.kan_s = KANBlock(hidden_dim)

        # 特征融合层 (需要根据 fusion_method 实现具体融合逻辑)
        # 如果是 concat, 输入通道数会翻倍
        fusion_in_dim = hidden_dim * 2 if fusion_method == 'concat' else hidden_dim
        self.fusion_layer = nn.Conv2d(fusion_in_dim, hidden_dim, kernel_size=1) # 占位符
        
        # 如果使用 weighted 融合，创建权重融合模块
        if fusion_method == 'weighted':
            self.weighted_fusion = WeightedFeatureFusion(hidden_dim)

        # 回归头：预测密度图
        self.regressor = nn.Conv2d(hidden_dim, 1, kernel_size=1)

    def _downscale(self, img: torch.Tensor) -> torch.Tensor:
        return F.interpolate(img, scale_factor=self.downscale_factor, mode="bilinear", align_corners=False)

    def _central_crop(self, img: torch.Tensor) -> torch.Tensor:
        _, _, h, w = img.size()
        ch, cw = int(h * self.crop_ratio), int(w * self.crop_ratio)
        top = (h - ch) // 2
        left = (w - cw) // 2
        return img[:, :, top : top + ch, left : left + cw]

    def _extract_mobilenet_stages(self, x):
        # MobileNetV3 Large: features[5] (stage2, C=40), [11] (stage3, C=96), [16] (stage4, C=576)
        feats = []
        out = x
        for i, layer in enumerate(self.encoder.features):
            out = layer(out)
            if i == 5:  # stage2 output (C=40)
                feats.append(out)
            if i == 11:  # stage3 output (C=96)
                feats.append(out)
        feats.append(out)  # stage4 output (C=576)
        return feats  # [stage2, stage3, stage4]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        input_size = x.shape[-2:]

        if self.use_msff:
            # 多尺度特征融合分支（以下采样分支为例）
            x_ds = self._downscale(x)
            feats = self._extract_mobilenet_stages(x_ds)
            f2 = self.msff_conv2(feats[0])  # [B, hidden_dim, H/4, W/4]
            f3 = F.interpolate(self.msff_conv3(feats[1]), size=f2.shape[-2:], mode="bilinear", align_corners=False)
            f4 = F.interpolate(self.msff_conv4(feats[2]), size=f2.shape[-2:], mode="bilinear", align_corners=False)
            fused = torch.cat([f2, f3, f4], dim=1)  # [B, hidden_dim*3, H/4, W/4]
            fused = self.msff_fuse(fused)           # [B, hidden_dim, H/4, W/4]
            # 使用为 MSFF 分支定义的 KAN 块
            fused = self.msff_kan(fused)
            # 使用为 MSFF 分支定义的回归头
            density_map = self.msff_regressor(fused)

            # # 上采样 (如果需要，可能 MSFF 分支不需要复杂的上采样)
            # x = F.relu(self.up1(fused))
            if density_map.shape[-2:] != input_size:
                density_map = F.interpolate(density_map, size=input_size, mode="bilinear", align_corners=False)
            return density_map
        else:
            # 原有双分支结构 (更新逻辑)
            # Branch 1: Downscaling
            x_ds = self._downscale(x)
            feat_a_spatial = self.encoder.features(x_ds)
            feat_a = self.reduce(feat_a_spatial) # [B, hidden_dim, H', W']
            feat_a = self.kan_a(feat_a)           # Apply KAN to branch A

            # Branch 2: Cropping
            x_cp = self._central_crop(x)
            # Resize cropped image to match downscaled feature map size
            x_cp_resized = F.interpolate(x_cp, size=feat_a.shape[-2:], mode="bilinear", align_corners=False)
            feat_s_spatial = self.encoder.features(x_cp_resized)
            feat_s = self.reduce(feat_s_spatial) # [B, hidden_dim, H', W']
            feat_s = self.kan_s(feat_s)           # Apply KAN to branch S

            # Apply Attention (if enabled)
            if self.use_attention:
                # TODO: Implement specific attention based on self.attention_type
                # Example: feat_a = self.attention(feat_a)
                #          feat_s = self.attention(feat_s)
                feat_a = self.attention(feat_a)
                feat_s = self.attention(feat_s)

            # Feature Fusion
            if self.fusion_method == 'concat':
                fused = torch.cat([feat_a, feat_s], dim=1) # [B, hidden_dim*2, H', W']
            elif self.fusion_method == 'add':
                fused = feat_a + feat_s # [B, hidden_dim, H', W']
            elif self.fusion_method == 'weighted':
                # 使用可学习的加权融合
                fused = self.weighted_fusion(feat_a, feat_s)
            else: # Default to add
                fused = feat_a + feat_s

            # Apply fusion layer (adjusts channels after concat)
            fused = self.fusion_layer(fused) # [B, hidden_dim, H', W']

            # Regression Head
            density_map = self.regressor(fused) # [B, 1, H', W']

            # Upsample to original input size
            if density_map.shape[-2:] != input_size:
                density_map = F.interpolate(density_map, size=input_size, mode="bilinear", align_corners=False)

            # Apply activation (e.g., ReLU or Softplus)
            # Using ReLU as a common choice for density maps
            density_map = F.relu(density_map)

            return density_map

# -----------------------------------------------------------------------------
# Debug run
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    model_cc = LARIQACrowdCounter()
    dummy = torch.randn(1, 3, 512, 512)
    out_map = model_cc(dummy)
    print("Density map shape:", out_map.shape) 